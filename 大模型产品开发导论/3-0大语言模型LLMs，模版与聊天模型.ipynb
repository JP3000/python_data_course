{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三节 大语言模型LLMs，模版与聊天模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大语言模型llm的常用参数与函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入三个模型各自的key\n",
    "\n",
    "import os\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_community.llms import SparkLLM\n",
    "from langchain_community.llms import QianfanLLMEndpoint\n",
    "\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-9bcefecc91cb4a20b04c5cabce8963dc\"\n",
    "\n",
    "os.environ[\"IFLYTEK_SPARK_APP_ID\"] = \"\"\n",
    "os.environ[\"IFLYTEK_SPARK_API_KEY\"] = \"\"\n",
    "os.environ[\"IFLYTEK_SPARK_API_SECRET\"] = \"\"\n",
    "\n",
    "os.environ[\"QIANFAN_AK\"] = \"\"\n",
    "os.environ[\"QIANFAN_SK\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 参数 Temperature\n",
    "\n",
    "Temperature: 用于调整随机从生成模型中抽样的程度，因此每次“生成”时，相同的提示可能会产生不同的输出。温度为 0 将始终产生相同的输出。温度越高随机性越大！主要用于控制创造力。\n",
    "\n",
    "LLM中的一个耐人寻味的参数是 「温度」。「温度」是一个用于调整模型生成文本时创造性和多样性的超参数。「温度」是一个大于0的数值，通常在 0 到 1 之间。它影响模型生成文本时采样预测词汇的概率分布。当模型的「温度」较高时（如 0.8、1 或更高），模型会更倾向于从较多样且不同的词汇中选择，这使得生成的文本风险性更高、创意性更强，但也可能产生更多的错误和不连贯之处。而当「温度」较低时（如 0.2、0.3 等），模型主要会从具有较高概率的词汇中选择，从而产生更平稳、更连贯的文本。但此时，生成的文本可能会显得过于保守和重复。因此在实际应用中，需要根据具体需求来权衡选择合适的「温度」值。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/666670367"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，让我们调用通义千问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tongyi_di=Tongyi(temperature=0.1)\n",
    "llm_tongyi_zh=Tongyi(temperature=0.5)\n",
    "llm_tongyi_ga=Tongyi(temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt0=\"请写一首有关大理的四言诗\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大理风光，山川秀美。\n",
      "洱海映月，苍山雪翠。\n",
      "古道西风，瘦马寻梅。\n",
      "玉龙蜿蜒，云影徘徊。\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_di.invoke(prompt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大理风光，山川秀丽。\n",
      "洱海波光，苍山雪霁。\n",
      "古城悠悠，岁月静好。\n",
      "风花雪月，心旷神怡。\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_zh.invoke(prompt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大理风光，山川秀丽。\n",
      "洱海明珠，苍山玉带。\n",
      "古道西风，瘦马行旅。\n",
      "白墙青瓦，民风淳美。\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_ga.invoke(prompt0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数streaming——流式输出\n",
    "\n",
    "* 通义千问和星火似乎都不支持下面langchain中文文档中的流式输出模式，只有文心一言能用\n",
    "https://python.langchain.com.cn/docs/modules/model_io/models/chat/streaming\n",
    "\n",
    "* 由于发现zhipu ai有比较好的embeddings功能，可能使用zhipu ai作为向量数据库存储的主要模型，所以在此试试langchain的zhipu ai的流式输出模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "llm_zhipu = ChatZhipuAI(\n",
    "    temperature=0.5,\n",
    "    api_key=\"a6ed0abe5933434b8ae29d44ed4997ba.frKjIq8FKLpKuIlq\",\n",
    "    model=\"glm-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='《梦绕西湖》\\n\\n（第一段）\\n踏遍千山万水，独醉西湖美景\\n碧波荡漾，风轻云淡，如诗如画如梦境\\n烟雨江南，水墨丹青，浓墨重彩难画情\\n\\n（副歌）\\n梦绕西湖，心恋江南水乡\\n白蛇传奇，断桥残雪，千古传颂\\n渔舟唱晚，夕阳映照，岁月静好\\n此生此景，不负如来不负卿\\n\\n（第二段）\\n漫步苏堤春晓，桃红柳绿映水间\\n芳草萋萋，莺歌燕舞，春意盎然\\n雷峰塔下，夕照金山，时光荏苒换新颜\\n\\n（副歌）\\n梦绕西湖，心恋江南水乡\\n白蛇传奇，断桥残雪，千古传颂\\n渔舟唱晚，夕阳映照，岁月静好\\n此生此景，不负如来不负卿\\n\\n（桥段）\\n碧水映晴空，风起云涌，似仙子舞动长袖\\n诗酒趁年华，共度良辰，让时光倒流\\n江南烟雨，梦回西湖，那年的约定\\n手牵手，共赴一场美丽的邂逅\\n\\n（副歌）\\n梦绕西湖，心恋江南水乡\\n白蛇传奇，断桥残雪，千古传颂\\n渔舟唱晚，夕阳映照，岁月静好\\n此生此景，不负如来不负卿\\n\\n（尾声）\\n梦绕西湖，恋恋不舍，离别总在日出时\\n愿岁月静好，佳人依旧，共度余生\\n让这美景，化作永恒，留在心间\\n梦中相见，那片美丽的西湖山水', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'token_usage': {'prompt_tokens': 11, 'completion_tokens': 367, 'total_tokens': 378}, 'model_name': 'glm-4'}, id='run--c210eb0a-8787-4eff-ad8c-25973c70dfc9')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = llm_zhipu.stream(\"帮我写一首有关西湖的歌\")\n",
    "full = next(stream)\n",
    "for chunk in stream:\n",
    "    full += chunk\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "llm_wx_liu = QianfanLLMEndpoint(streaming=True, callbacks=[StreamingStdOutCallbackHandler()],temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-04 20:22:55] openapi_requestor.py:316 [t:3588]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，以下是一首关于西湖的歌：\n",
      "\n",
      "西湖之歌\n",
      "\n",
      "(诗词)\n",
      "\n",
      "水光潋滟晴方好，山色空蒙雨亦奇。\n",
      "欲把西湖比西子，淡妆浓抹总相宜。\n",
      "\n",
      "(副歌)\n",
      "\n",
      "西湖之歌，婉约又豪放，\n",
      "碧波荡漾，情深意长。\n",
      "柳丝轻舞，花儿笑开，\n",
      "西湖之美，让人心醉。\n",
      "\n",
      "(歌词)\n",
      "\n",
      "第一节：\n",
      "晨曦微露，西湖醒来，\n",
      "薄雾轻绕，如梦如幻。\n",
      "白鹭低飞，鱼儿跃起，\n",
      "碧波荡漾，心旷神怡。\n",
      "\n",
      "副歌：\n",
      "西湖之歌，婉约又豪放，\n",
      "碧波荡漾，情深意长。\n",
      "月下西湖，静谧又神秘，\n",
      "夜色西湖，如诗如画。\n",
      "\n",
      "第二节：\n",
      "雷峰塔倒，白娘子的传说，\n",
      "断桥残雪，千古传颂。\n",
      "苏堤春晓，花开如画，\n",
      "三潭印月，美不胜收。\n",
      "\n",
      "副歌：\n",
      "西湖之歌，婉约又豪放，\n",
      "碧波荡漾，情深意长。\n",
      "风吹湖面，涟漪荡漾，\n",
      "心随西湖，飘向远方。\n",
      "\n",
      "第三节和尾声：\n",
      "西湖之美，四季如画，\n",
      "春花秋月，夏阳冬雪。\n",
      "走过岁月，西湖依旧，\n",
      "我心中的西湖之歌，永远唱响。\n",
      "\n",
      "这首歌描绘了西湖的美丽景色和传说，表达了对西湖的深深喜爱和向往。希望你喜欢！"
     ]
    }
   ],
   "source": [
    "resp = llm_wx_liu.invoke(\"帮我写一首有关西湖的歌\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数get_num_tokens————计算token数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm_tongyi_ga.get_num_tokens(\u001b[33m\"\u001b[39m\u001b[33mfasfdsafreqwrwe\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_core/language_models/base.py:366\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[32m    357\u001b[39m \n\u001b[32m    358\u001b[39m \u001b[33;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m \u001b[33;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.get_token_ids(text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_core/language_models/base.py:353\u001b[39m, in \u001b[36mBaseLanguageModel.get_token_ids\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids(text)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_token_ids_default_method(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_core/language_models/base.py:79\u001b[39m, in \u001b[36m_get_token_ids_default_method\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m tokenizer = get_tokenizer()\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.encode(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_core/language_models/base.py:73\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GPT2TokenizerFast.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2024\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2022\u001b[39m             resolved_vocab_files[file_id] = download_url(file_path, proxies=proxies)\n\u001b[32m   2023\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m         resolved_vocab_files[file_id] = cached_file(\n\u001b[32m   2025\u001b[39m             pretrained_model_name_or_path,\n\u001b[32m   2026\u001b[39m             file_path,\n\u001b[32m   2027\u001b[39m             cache_dir=cache_dir,\n\u001b[32m   2028\u001b[39m             force_download=force_download,\n\u001b[32m   2029\u001b[39m             proxies=proxies,\n\u001b[32m   2030\u001b[39m             resume_download=resume_download,\n\u001b[32m   2031\u001b[39m             local_files_only=local_files_only,\n\u001b[32m   2032\u001b[39m             token=token,\n\u001b[32m   2033\u001b[39m             user_agent=user_agent,\n\u001b[32m   2034\u001b[39m             revision=revision,\n\u001b[32m   2035\u001b[39m             subfolder=subfolder,\n\u001b[32m   2036\u001b[39m             _raise_exceptions_for_gated_repo=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2037\u001b[39m             _raise_exceptions_for_missing_entries=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2038\u001b[39m             _raise_exceptions_for_connection_errors=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2039\u001b[39m             _commit_hash=commit_hash,\n\u001b[32m   2040\u001b[39m         )\n\u001b[32m   2041\u001b[39m         commit_hash = extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[32m   2043\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2044\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         hf_hub_download(\n\u001b[32m    425\u001b[39m             path_or_repo_id,\n\u001b[32m    426\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m    427\u001b[39m             subfolder=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[32m    428\u001b[39m             repo_type=repo_type,\n\u001b[32m    429\u001b[39m             revision=revision,\n\u001b[32m    430\u001b[39m             cache_dir=cache_dir,\n\u001b[32m    431\u001b[39m             user_agent=user_agent,\n\u001b[32m    432\u001b[39m             force_download=force_download,\n\u001b[32m    433\u001b[39m             proxies=proxies,\n\u001b[32m    434\u001b[39m             resume_download=resume_download,\n\u001b[32m    435\u001b[39m             token=token,\n\u001b[32m    436\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    437\u001b[39m         )\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    942\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    943\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    959\u001b[39m     )\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[32m    962\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    963\u001b[39m         cache_dir=cache_dir,\n\u001b[32m    964\u001b[39m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[32m    965\u001b[39m         repo_id=repo_id,\n\u001b[32m    966\u001b[39m         filename=filename,\n\u001b[32m    967\u001b[39m         repo_type=repo_type,\n\u001b[32m    968\u001b[39m         revision=revision,\n\u001b[32m    969\u001b[39m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[32m    970\u001b[39m         endpoint=endpoint,\n\u001b[32m    971\u001b[39m         etag_timeout=etag_timeout,\n\u001b[32m    972\u001b[39m         headers=hf_headers,\n\u001b[32m    973\u001b[39m         proxies=proxies,\n\u001b[32m    974\u001b[39m         token=token,\n\u001b[32m    975\u001b[39m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[32m    976\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    977\u001b[39m         force_download=force_download,\n\u001b[32m    978\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1024\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = _get_metadata_or_catch_error(\n\u001b[32m   1025\u001b[39m     repo_id=repo_id,\n\u001b[32m   1026\u001b[39m     filename=filename,\n\u001b[32m   1027\u001b[39m     repo_type=repo_type,\n\u001b[32m   1028\u001b[39m     revision=revision,\n\u001b[32m   1029\u001b[39m     endpoint=endpoint,\n\u001b[32m   1030\u001b[39m     proxies=proxies,\n\u001b[32m   1031\u001b[39m     etag_timeout=etag_timeout,\n\u001b[32m   1032\u001b[39m     headers=headers,\n\u001b[32m   1033\u001b[39m     token=token,\n\u001b[32m   1034\u001b[39m     local_files_only=local_files_only,\n\u001b[32m   1035\u001b[39m     storage_folder=storage_folder,\n\u001b[32m   1036\u001b[39m     relative_filename=relative_filename,\n\u001b[32m   1037\u001b[39m )\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1047\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1050\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1484\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m         metadata = get_hf_file_metadata(\n\u001b[32m   1485\u001b[39m             url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token\n\u001b[32m   1486\u001b[39m         )\n\u001b[32m   1487\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1488\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1489\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1401\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1398\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m r = _request_wrapper(\n\u001b[32m   1402\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mHEAD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1403\u001b[39m     url=url,\n\u001b[32m   1404\u001b[39m     headers=hf_headers,\n\u001b[32m   1405\u001b[39m     allow_redirects=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1406\u001b[39m     follow_relative_redirects=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1407\u001b[39m     proxies=proxies,\n\u001b[32m   1408\u001b[39m     timeout=timeout,\n\u001b[32m   1409\u001b[39m )\n\u001b[32m   1410\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:285\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     response = _request_wrapper(\n\u001b[32m    286\u001b[39m         method=method,\n\u001b[32m    287\u001b[39m         url=url,\n\u001b[32m    288\u001b[39m         follow_relative_redirects=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    289\u001b[39m         **params,\n\u001b[32m    290\u001b[39m     )\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:308\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m    309\u001b[39m hf_raise_for_status(response)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28mself\u001b[39m.send(prep, **send_kwargs)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = adapter.send(request, **kwargs)\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().send(request, *args, **kwargs)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = conn.urlopen(\n\u001b[32m    668\u001b[39m         method=request.method,\n\u001b[32m    669\u001b[39m         url=url,\n\u001b[32m    670\u001b[39m         body=request.body,\n\u001b[32m    671\u001b[39m         headers=request.headers,\n\u001b[32m    672\u001b[39m         redirect=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    673\u001b[39m         assert_same_host=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    674\u001b[39m         preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    675\u001b[39m         decode_content=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    676\u001b[39m         retries=\u001b[38;5;28mself\u001b[39m.max_retries,\n\u001b[32m    677\u001b[39m         timeout=timeout,\n\u001b[32m    678\u001b[39m         chunked=chunked,\n\u001b[32m    679\u001b[39m     )\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    788\u001b[39m     conn,\n\u001b[32m    789\u001b[39m     method,\n\u001b[32m    790\u001b[39m     url,\n\u001b[32m    791\u001b[39m     timeout=timeout_obj,\n\u001b[32m    792\u001b[39m     body=body,\n\u001b[32m    793\u001b[39m     headers=headers,\n\u001b[32m    794\u001b[39m     chunked=chunked,\n\u001b[32m    795\u001b[39m     retries=retries,\n\u001b[32m    796\u001b[39m     response_conn=response_conn,\n\u001b[32m    797\u001b[39m     preload_content=preload_content,\n\u001b[32m    798\u001b[39m     decode_content=decode_content,\n\u001b[32m    799\u001b[39m     **response_kw,\n\u001b[32m    800\u001b[39m )\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_conn(conn)\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     conn.connect()\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/connection.py:704\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    703\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28mself\u001b[39m._new_conn()\n\u001b[32m    705\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    706\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = connection.create_connection(\n\u001b[32m    199\u001b[39m         (\u001b[38;5;28mself\u001b[39m._dns_host, \u001b[38;5;28mself\u001b[39m.port),\n\u001b[32m    200\u001b[39m         \u001b[38;5;28mself\u001b[39m.timeout,\n\u001b[32m    201\u001b[39m         source_address=\u001b[38;5;28mself\u001b[39m.source_address,\n\u001b[32m    202\u001b[39m         socket_options=\u001b[38;5;28mself\u001b[39m.socket_options,\n\u001b[32m    203\u001b[39m     )\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_env/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m sock.connect(sa)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 需要安装PyTorch\n",
    "llm_tongyi_ga.get_num_tokens(\"fasfdsafreqwrwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数generate————多个提示词的支持\n",
    "\n",
    "* 注意对应信息获取方式\n",
    "* 该方法依然能获取到tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0=llm_tongyi_di.generate([\"写一首四句的诗\",\"写一幅对联\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='春来花开满园香，\\n夏至蝉鸣绿荫长。\\n秋风送爽叶舞黄，\\n冬雪皑皑炉火旺。', generation_info={'finish_reason': 'stop', 'request_id': '8f94717f-e9f5-98c5-824a-6d904db702f0', 'token_usage': {'input_tokens': 14, 'output_tokens': 31, 'total_tokens': 45, 'prompt_tokens_details': {'cached_tokens': 0}}})], [Generation(text='上联：春风得意马蹄疾  \\n下联：岁月峥嵘人未老  \\n\\n横批：前程似锦', generation_info={'finish_reason': 'stop', 'request_id': 'dca26296-bc04-9275-a843-ae8451219968', 'token_usage': {'input_tokens': 12, 'output_tokens': 26, 'total_tokens': 38, 'prompt_tokens_details': {'cached_tokens': 0}}})]], llm_output={'model_name': 'qwen-plus'}, run=[RunInfo(run_id=UUID('a843ad3b-f5af-4a2e-908a-f03afe606637')), RunInfo(run_id=UUID('c30477a2-3eb4-46ac-89a9-22a89c397cea'))], type='LLMResult')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'request_id': '8f94717f-e9f5-98c5-824a-6d904db702f0',\n",
       " 'token_usage': {'input_tokens': 14,\n",
       "  'output_tokens': 31,\n",
       "  'total_tokens': 45,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res0.generations[0][0].generation_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classwork-1\n",
    "\n",
    "1. 让大模型帮忙写一幅对联，用get_num_tokens获得此提示词的token数量\n",
    "\n",
    "2. 改变模型的温度，看看对联是否有不一样？\n",
    "\n",
    "3. 通过流式输出上述对联的结果\n",
    "\n",
    "4. 通过列表定义任意几个提示词，通过generate获得多个提示词的生成结果, 并获得这些结果的total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 低温度 (Low Temperature) ===\n",
      "上联：春风得意马蹄疾  \n",
      "下联：岁月峥嵘人未老  \n",
      "\n",
      "横批：前程似锦\n",
      "\n",
      "=== 中温度 (Medium Temperature) ===\n",
      "上联：春风得意马蹄疾  \n",
      "下联：岁月峥嵘人未老  \n",
      "\n",
      "横批：前程似锦\n",
      "\n",
      "=== 高温度 (High Temperature) ===\n",
      "上联：春风得意马蹄疾  \n",
      "下联：岁月峥嵘人未老  \n",
      "\n",
      "横批：前程似锦\n"
     ]
    }
   ],
   "source": [
    "# 模型低、中、高的 temperature 生成的结果\n",
    "low_temp_result = llm_tongyi_di.invoke(\"帮我写一幅对联\")\n",
    "mid_temp_result = llm_tongyi_zh.invoke(\"帮我写一幅对联\")\n",
    "high_temp_result = llm_tongyi_ga.invoke(\"帮我写一幅对联\")\n",
    "\n",
    "# 格式化输出\n",
    "print(\"=== 低温度 (Low Temperature) ===\")\n",
    "print(low_temp_result)\n",
    "print(\"\\n=== 中温度 (Medium Temperature) ===\")\n",
    "print(mid_temp_result)\n",
    "print(\"\\n=== 高温度 (High Temperature) ===\")\n",
    "print(high_temp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='上联：春风得意马蹄疾  \\n下联：岁月峥嵘人未老  \\n\\n横批：前程似锦', generation_info={'finish_reason': 'stop', 'request_id': 'f83616c5-8bcc-90a8-a18f-ae1a078b999a', 'token_usage': {'input_tokens': 13, 'output_tokens': 26, 'total_tokens': 39, 'prompt_tokens_details': {'cached_tokens': 0}}})], [Generation(text='锦官城外柏森森，遥望西岭雪皑皑。\\n成都平原风光好，花重锦官城自开。\\n\\n草堂遗址今犹在，杜甫诗魂永不衰。\\n武侯祠前思贤相，青羊宫里祈年来。\\n\\n春熙路上人潮涌，宽窄巷中古韵来。\\n火锅飘香满街巷，川剧变脸惊四海。\\n\\n夜幕降临锦江畔，灯火辉煌映水来。\\n九天开出一成都，万户千门入画图。', generation_info={'finish_reason': 'stop', 'request_id': '5117a991-bae9-9dd6-adcf-78934d2022af', 'token_usage': {'input_tokens': 14, 'output_tokens': 114, 'total_tokens': 128, 'prompt_tokens_details': {'cached_tokens': 0}}})]], llm_output={'model_name': 'qwen-plus'}, run=[RunInfo(run_id=UUID('9c490e80-2707-4e1d-8983-5263c4db34a0')), RunInfo(run_id=UUID('47da0b9f-676f-42f8-a5fe-bb1655dc6ce5'))], type='LLMResult')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classwork_res = llm_tongyi_di.generate([\"帮我写一幅对联\",\"写一首关于成都的诗\"])\n",
    "classwork_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 13,\n",
       " 'output_tokens': 26,\n",
       " 'total_tokens': 39,\n",
       " 'prompt_tokens_details': {'cached_tokens': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看tokens\n",
    "type(classwork_res.generations[0][0])\n",
    "classwork_res.generations[0][0].generation_info['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模版的使用\n",
    "\n",
    "### 简单模版\n",
    "\n",
    "语言模型以文本作为输入，这段文本通常被称为提示（Prompt）。通常情况下，这不仅仅是一个硬编码的字符串，而是模板、示例和用户输入的组合。LangChain提供了多个类和函数，以便轻松构建和处理提示。\n",
    "\n",
    "提示模板是指一种可复制的生成提示的方式。它包含一个文本字符串（模板），可以从最终用户处接收一组参数并生成提示。提示模板可能包含以下内容：\n",
    "\n",
    "* 对语言模型的指令\n",
    "* 少量示例，以帮助语言模型生成更好的回复\n",
    "* 对语言模型的问题\n",
    "\n",
    "原文链接： https://machinelearning.blog.csdn.net/article/details/131988052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n我希望你能充当新公司的命名顾问。\\n一个生产彩色袜子的公司的好名字是什么？\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "我希望你能充当新公司的命名顾问。\n",
    "一个生产{product}的公司的好名字是什么？\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt.format(product=\"彩色袜子\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为一家生产彩色袜子的公司命名时，我们希望名字能够体现色彩、趣味和时尚感，同时易于记忆和传播。以下是一些创意名称供你参考：\n",
      "\n",
      "### 1. **ColorSocks**\n",
      "   - 简洁明了，直接传达“彩色袜子”的核心理念。\n",
      "   - 可以搭配标语如“让每一天都充满色彩”。\n",
      "\n",
      "### 2. **RainbowFeet**\n",
      "   - “Rainbow”象征丰富的颜色，“Feet”则点明产品用途。\n",
      "   - 给人活泼、年轻的感觉。\n",
      "\n",
      "### 3. **Socktastic**\n",
      "   - 结合“sock”（袜子）与“fantastic”（奇妙的），创造一个有趣且独特的词汇。\n",
      "   - 强调产品的独特性和吸引力。\n",
      "\n",
      "### 4. **Chrom Sox**\n",
      "   - “Chrom”来源于“chromatic”（色彩缤纷的），与“sox”（袜子的简写）结合。\n",
      "   - 科技感与艺术感并存。\n",
      "\n",
      "### 5. **HappyHeels**\n",
      "   - 着重于穿着者的心情，“Happy”传递积极情绪，“Heels”可以理解为脚后跟或整体脚部。\n",
      "   - 温暖而亲切。\n",
      "\n",
      "### 6. **Artful Socks**\n",
      "   - 将袜子比作艺术品，展现设计的独特性。\n",
      "   - 非常适合强调个性化和创意图案的品牌。\n",
      "\n",
      "### 7. **Pigment Feet**\n",
      "   - “Pigment”意为颜料，突出了袜子的颜色丰富性。\n",
      "   - 名字现代且有深度。\n",
      "\n",
      "### 8. **BriteSocks**\n",
      "   - “Brite”是“bright”（明亮的）的变体拼写，寓意鲜艳的颜色。\n",
      "   - 易记且富有活力。\n",
      "\n",
      "### 9. **Sole Colors**\n",
      "   - “Sole”既指“灵魂”，也指“鞋底”，双关语增加了趣味性。\n",
      "   - 表达每双袜子都有其独特的灵魂和色彩。\n",
      "\n",
      "### 10. **Crazy Threads**\n",
      "   - “Threads”泛指衣物纤维，“Crazy”体现大胆的设计风格。\n",
      "   - 适合走潮流路线的品牌。\n",
      "\n",
      "如果你需要更具体的建议，比如目标市场、品牌定位或其他元素，请告诉我！这样我可以进一步优化名字方案。\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_zh.invoke(prompt.format(product=\"彩色袜子\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'给我讲一个有趣的的关于小鸡的中文笑话。'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多个输入变量的示例提示\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"], \n",
    "    template=\"给我讲一个{adjective}的关于{content}的中文笑话。\"\n",
    ")\n",
    "multiple_input_prompt.format(adjective=\"有趣的\", content=\"小鸡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！这是一个关于小鸡的有趣笑话：\n",
      "\n",
      "有一天，一只小鸡去参加考试。考卷上第一题是：“蛋是从哪里来的？”\n",
      "\n",
      "小鸡想了想，在答案栏里写道：“从我肚子里出来的。”\n",
      "\n",
      "老师改卷时看到这个答案，哭笑不得，便在旁边批注：“错！请回去问你的妈妈。”\n",
      "\n",
      "小鸡回家后把老师的评语给鸡妈妈看，鸡妈妈看完后严肃地对小鸡说：“孩子，以后这种问题你就直接写‘树上掉下来的’，明白了吗？”\n",
      "\n",
      "希望你喜欢这个笑话！😄\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_zh.invoke(multiple_input_prompt.format(adjective=\"有趣的\", content=\"小鸡\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  包含例子的模版\n",
    "\n",
    "向模板添加Few Shot示例\n",
    "Few Shot示例是一组可以帮助语言模型生成更好响应的示例。要使用Few Shot示例生成提示，可以使用FewShotPromptTemplate。此类接受PromptTemplate和Few Shot示例列表。然后，它使用Few Shot示例格式化提示模板。\n",
    "\n",
    "在下面示例中，我们将创建一个生成单词反义词的提示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: 正常\n",
      "Antonym: 反常\n",
      "\n",
      "Word: big\n",
      "Antonym: \n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# 首先，创建Few Shot示例列表\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"正常\", \"antonym\": \"反常\"},\n",
    "]\n",
    "\n",
    "# 接下来，我们指定用于格式化示例的模板。\n",
    "# 我们使用`PromptTemplate`类来实现这个目的。\n",
    "\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# 最后，创建`FewShotPromptTemplate`对象。\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # 这些是我们要插入到提示中的示例。\n",
    "    examples=examples,\n",
    "    # 这是我们在将示例插入到提示中时要使用的格式。\n",
    "    example_prompt=example_prompt,\n",
    "    # 前缀是出现在提示中示例之前的一些文本。\n",
    "    # 通常，这包括一些说明。\n",
    "    prefix=\"Give the antonym of every input\\n\",\n",
    "    # 后缀是出现在提示中示例之后的一些文本。\n",
    "    # 通常，这是用户输入的地方。\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    # 输入变量是整个提示期望的变量。\n",
    "    input_variables=[\"input\"],\n",
    "    # 示例分隔符是我们将前缀、示例和后缀连接在一起的字符串。\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "# 现在，我们可以使用`format`方法生成一个提示。\n",
    "print(few_shot_prompt.format(input=\"big\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tongyi_zh.invoke(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'丑陋'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    {\"word\": \"正常\", \"antonym\": \"反常\"} 在加入该中文例子后，会有中文对应反义词输出\n",
    "llm_tongyi_di(few_shot_prompt.format(input=\"美丽\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classwork-2\n",
    "\n",
    "1. 通过引入带 __变量__ 的 __模板__ ，控制对联的类型，以此输出满足不同需求的对联\n",
    "\n",
    "2. 使用带 __例子__ 的 __模板__ 实现一个自动对对联工具，要求你给出上联，大模型给出对应下联，例子自己百度\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n你是一个中文文人，我希望你能帮我写一幅关于成都的对联。\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.带变量的模版\n",
    "from langchain import PromptTemplate\n",
    "template = \"\"\"\n",
    "你是一个中文文人，我希望你能帮我写一幅关于{input}的对联。\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "prompt.format(input=\"成都\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上联：山城雾绕，巴渝古韵千秋画  \n",
      "下联：江水流长，重庆新风万里歌  \n",
      "\n",
      "横批：山河壮丽  \n",
      "\n",
      "这幅对联中，\"山城雾绕\"描绘了重庆多雾的自然特色，\"巴渝古韵千秋画\"体现了重庆悠久的历史文化；\"江水流长\"指长江与嘉陵江在重庆交汇，象征城市的发展源远流长，\"重庆新风万里歌\"则展现了现代重庆蓬勃发展的新风貌。横批\"山河壮丽\"概括赞美了重庆美丽的山水景色。\n"
     ]
    }
   ],
   "source": [
    "# 使用大模型 回答 提示词模版的提示内容\n",
    "print(llm_tongyi_zh.invoke(prompt.format(input=\"重庆\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一位中国的文人，给我写一幅对联\n",
      "\n",
      "上联: 古典精华凝诗句\n",
      "下联: 现代风貌入画屏\n",
      "横批: 文化传承\n",
      "\n",
      "上联: 月明点滴窗前影\n",
      "下联: 风清摇曳树下声\n",
      "横批: 自然之美\n",
      "\n",
      "上联: 春风化雨润心田\n",
      "下联: 夏日炎炎照大地\n",
      "横批: 四季交替\n",
      "\n",
      "上联: 古典精华凝诗句\n",
      "下联: \n"
     ]
    }
   ],
   "source": [
    "# 2.使用带 例子 的 模版 实现一个自动对对联工具，要求你给出上联，大模型给出对应下联\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "# 首先，创建Few Shot示例列表\n",
    "examples = [\n",
    "    {\"上联\":\"古典精华凝诗句\",\"下联\":\"现代风貌入画屏\", \"横批\":\"文化传承\"},\n",
    "    {\"上联\":\"月明点滴窗前影\",\"下联\":\"风清摇曳树下声\", \"横批\":\"自然之美\"},\n",
    "    {\"上联\":\"春风化雨润心田\",\"下联\":\"夏日炎炎照大地\", \"横批\":\"四季交替\"},\n",
    "]\n",
    "\n",
    "# 接下来，我们指定用于格式化示例的模板。\n",
    "# 我们使用`PromptTemplate`类来实现这个目的。\n",
    "example_formatter_template = \"\"\"上联: {上联}\n",
    "下联: {下联}\n",
    "横批: {横批}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"上联\", \"下联\", \"横批\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "# 最后，创建`FewShotPromptTemplate`对象。\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # 这些是我们要插入到提示中的示例。\n",
    "    examples=examples,\n",
    "    # 这是我们在将示例插入到提示中时要使用的格式。\n",
    "    example_prompt=example_prompt,\n",
    "    # 前缀是出现在提示中示例之前的一些文本。\n",
    "    # 通常，这包括一些说明。\n",
    "    prefix=\"你是一位中国的文人，给我写一幅对联\\n\",\n",
    "    # 后缀是出现在提示中示例之后的一些文本。\n",
    "    # 通常，这是用户输入的地方。\n",
    "    suffix=\"上联: {input}\\n下联: 横批: \",\n",
    "    # 输入变量是整个提示期望的变量。\n",
    "    input_variables=[\"input\"],\n",
    "    # 示例分隔符是我们将前缀、示例和后缀连接在一起的字符串。\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "# 现在，我们可以使用`format`方法生成一个提示。\n",
    "print(few_shot_prompt.format(input=\"古典精华凝诗句\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雾罩树林松\n",
      "\n",
      "横批: 景色朦胧 \n",
      "\n",
      "解析：上联“烟锁池塘柳”描绘出烟雾缭绕着池塘边的柳树，营造出一种朦胧的意境。下联“雾罩树林松”则对应地写出雾气笼罩着树林中的松树，与上联在景象上对仗工整，都展现了一种模糊而又静谧的大自然画面。“景色朦胧”作为横批，恰如其分地概括了这种因烟雾而产生的独特美感。\n"
     ]
    }
   ],
   "source": [
    "print(llm_tongyi_zh.invoke(few_shot_prompt.format(input=\"烟锁池塘柳\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天模型-chat_models\n",
    "\n",
    "聊天模型是语言模型的一种变体。虽然聊天模型在内部使用语言模型，但它们公开的接口略有不同。它们不是提供一个“输入文本，输出文本”的API，而是提供一个以“聊天消息”作为输入和输出的接口。 聊天模型的API还比较新，因此我们仍在确定正确的抽象层次。本问将介绍如何开始使用聊天模型，该接口是基于消息而不是原始文本构建的\n",
    "\n",
    "通过向聊天模型传递一个或多个消息，可以获取聊天完成的结果。响应将是一个消息。LangChain目前支持的消息类型有AIMessage、HumanMessage、SystemMessage和ChatMessage，其中ChatMessage接受一个任意的角色参数。大多数情况下，我们只需要处理HumanMessage、AIMessage和SystemMessage。\n",
    "                        \n",
    "原文链接：https://blog.csdn.net/hy592070616/article/details/131924904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通义千问，星火，文心一言的聊天模型实现\n",
    "\n",
    "#### 通义千问\n",
    "\n",
    "当前langchain版本（0.1.9）通义千问的流式接口有问题，暂时建议不用流式\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/tongyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatSparkLLM\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.chat_models import QianfanChatEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_ty = ChatTongyi(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chat_ty.stream([HumanMessage(content=\"hi\")], streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_ty.stream([HumanMessage(content=\"帮我写副对联\")], streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='上' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='联' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='：' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='春风送暖入' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='屠苏\\n下' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='联：瑞雪' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='迎春盈梅' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='香' additional_kwargs={} response_metadata={} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'request_id': 'f733cbca-dcf4-9d14-bc48-d2397784342a', 'token_usage': {'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'prompt_tokens_details': {'cached_tokens': 0}}} id='run--c582963d-b03b-47bc-8054-496e63be543c'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for chunk in res:\n",
    "        print(chunk)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='上联：春风送暖入屠苏\\n下联：瑞雪迎春兆丰年', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '3cbc5654-bef5-9e84-9b96-6ba2931a7788', 'token_usage': {'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--db843d66-f6c8-40a8-b337-d2c9747ec62c-0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ty = ChatTongyi()\n",
    "chat_ty.invoke([HumanMessage(content=\"帮我写副对联\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 星火大模型\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/sparkllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_xh = ChatSparkLLM()\n",
    "message = HumanMessage(content=\"Hello\")\n",
    "chat_xh.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?"
     ]
    }
   ],
   "source": [
    "# 流式调用\n",
    "chat_xh_s = ChatSparkLLM(streaming=True)\n",
    "for chunk in chat_xh_s.stream(\"Hello!\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 百度文心一言\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-04 22:27:40] openapi_requestor.py:316 [t:8028]: requesting llm api endpoint: /chat/eb-instant\n",
      "[INFO] [03-04 22:27:40] oauth.py:207 [t:8028]: trying to refresh access_token for ak `og6mWr***`\n",
      "[INFO] [03-04 22:27:40] oauth.py:220 [t:8028]: sucessfully refresh access_token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您好！有什么我可以帮助您的吗？')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_wx = QianfanChatEndpoint(streaming=True)\n",
    "messages = [HumanMessage(content=\"Hello\")]\n",
    "chat_wx.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-04 22:27:41] openapi_requestor.py:316 [t:8028]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！有什么我可以帮助您的吗？\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for chunk in chat_wx.stream(messages):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聊天模型的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_wx.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'c7b86e87-5adc-9687-a72e-41135d32958d', 'token_usage': {'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--1764d6bd-2679-4371-8ccc-b1b2191ab4ad-0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ty.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore programmer.\", generation_info={'finish_reason': 'stop', 'request_id': '73cc39b0-121e-9bdd-89bf-81cc990d4f0a', 'token_usage': {'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'prompt_tokens_details': {'cached_tokens': 0}}}, message=AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '73cc39b0-121e-9bdd-89bf-81cc990d4f0a', 'token_usage': {'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--9265307d-3395-4f30-8d04-f7dea1412ed3-0'))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info={'finish_reason': 'stop', 'request_id': '876152e4-9dc7-9a91-af84-bb06bc8ab5cd', 'token_usage': {'input_tokens': 29, 'output_tokens': 12, 'total_tokens': 41, 'prompt_tokens_details': {'cached_tokens': 0}}}, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '876152e4-9dc7-9a91-af84-bb06bc8ab5cd', 'token_usage': {'input_tokens': 29, 'output_tokens': 12, 'total_tokens': 41, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--945ab4fa-67ab-49f0-aa5a-89f313f32588-0'))]], llm_output={'model_name': 'qwen-turbo'}, run=[RunInfo(run_id=UUID('9265307d-3395-4f30-8d04-f7dea1412ed3')), RunInfo(run_id=UUID('945ab4fa-67ab-49f0-aa5a-89f313f32588'))], type='LLMResult')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat_ty.generate(batch_messages)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'qwen-turbo'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模版在聊天模型中的使用\n",
    "\n",
    "我们可以使用模板来构建MessagePromptTemplate。我们可以从一个或多个MessagePromptTemplate构建一个ChatPromptTemplate。我们还可以使用ChatPromptTemplate的format_prompt方法，它将返回一个PromptValue，我们可以将其转换为字符串或消息对象，具体取决于我们是否希望将格式化后的值作为输入传递给LLM或Chat模型的输入。为了方便起见，模板上公开了一个from_template方法。如果您要使用此模板，代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '58b630d2-cb12-9bc5-bb07-ebabcbbf105c', 'token_usage': {'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--d6f1a732-ac66-4ae8-8d66-f987342751b2-0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 获取格式化后的消息的聊天完成结果\n",
    "chat_ty.invoke(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classwork-3\n",
    "\n",
    "1. 基于聊天模型实现对对联工具，人给出上联，模型给出下联\n",
    "\n",
    "2. 使用模板实现用不同语言的对对联工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
